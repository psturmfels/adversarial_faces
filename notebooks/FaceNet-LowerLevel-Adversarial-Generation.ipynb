{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "import tensorflow as tf\n",
    "import glob\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from sklearn.metrics import pairwise_distances, roc_auc_score\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline\n",
    "from tqdm import tqdm\n",
    "import sys\n",
    "sys.path.append(\"../\")\n",
    "from inception_resnet_v1 import inference\n",
    "import pandas as pd\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer_to_target = \"Conv2d_4b_3x3\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prewhiten(x):\n",
    "    \"\"\"\n",
    "    A helper function to whiten an image, or a batch of images.\n",
    "    Args:\n",
    "        x: An image or batch of images.\n",
    "    \"\"\"\n",
    "    if x.ndim == 4:\n",
    "        axis = (1, 2, 3)\n",
    "        size = x[0].size\n",
    "    elif x.ndim == 3:\n",
    "        axis = (0, 1, 2)\n",
    "        size = x.size\n",
    "    else:\n",
    "        print(x.ndim)\n",
    "        raise ValueError('Dimension should be 3 or 4')\n",
    "\n",
    "    mean = np.mean(x, axis=axis, keepdims=True)\n",
    "    std = np.std(x, axis=axis, keepdims=True)\n",
    "#     std_adj = np.maximum(std, 1.0/np.sqrt(size))\n",
    "    y = (x - mean) / std\n",
    "    return y\n",
    "\n",
    "def l2_normalize(x, axis=-1, epsilon=1e-10):\n",
    "    \"\"\"\n",
    "    Normalizes an embedding to have unit length in the l2 metric.\n",
    "    Args:\n",
    "        x: A batch of numpy embeddings\n",
    "    \"\"\"\n",
    "    output = x / np.sqrt(np.maximum(np.sum(np.square(x),\n",
    "                                           axis=axis,\n",
    "                                           keepdims=True),\n",
    "                                    epsilon))\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PersonImages:\n",
    "    def __init__(self, person_name):\n",
    "        self.clean_folder = \"\"\n",
    "        self.clean_images = []\n",
    "        self.adversarial_images = []\n",
    "        self.orig_mean = None\n",
    "        self.orig_std = None\n",
    "        self.orig_paths = []\n",
    "        self.person_name = person_name\n",
    "    \n",
    "    def _load_one_facenet(self, path, resize_size=None, prewhiten_img=True):\n",
    "        img = Image.open(path).convert(\"RGB\")\n",
    "    \n",
    "            \n",
    "        if resize_size:\n",
    "            img = img.resize((resize_size, resize_size))\n",
    "            \n",
    "        img = (np.array(img)).astype(np.float32)\n",
    "        \n",
    "        if prewhiten_img:\n",
    "            img = prewhiten(img)\n",
    "            \n",
    "        return img\n",
    "        \n",
    "    def _load_folder_for_facenet(self, folder, resize_size=None):\n",
    "        paths_list = glob.glob(os.path.join(folder, \"*\"))\n",
    "        final_imgs = []\n",
    "        for img_path in paths_list:\n",
    "            final_imgs.append(\n",
    "                self._load_one_facenet(\n",
    "                    img_path, prewhiten_img=False, resize_size=resize_size))\n",
    "        \n",
    "        final_imgs = np.array(final_imgs)\n",
    "        mean, std = np.mean(final_imgs), np.std(final_imgs)\n",
    "        final_imgs = prewhiten(np.array(final_imgs))\n",
    "        return final_imgs, mean, std, paths_list\n",
    "        \n",
    "    def load_clean_from_folder(self, clean_folder, resize_size=160):\n",
    "        self.clean_folder = clean_folder\n",
    "        self.clean_images, self.orig_mean, self.orig_std, self.orig_paths = self._load_folder_for_facenet(\n",
    "            clean_folder, resize_size=resize_size)\n",
    "    \n",
    "    def _undo_preprocess(self, images):\n",
    "        restored_images = images.copy()\n",
    "        restored_images  *= self.orig_std\n",
    "        restored_images += self.orig_mean\n",
    "        restored_images = np.clip(restored_images, 0.0, 255.0)\n",
    "        return np.uint8(restored_images)\n",
    "    \n",
    "    def _compute_embeddings(self, model, images, layer_to_target):\n",
    "        return model.predict(np.array(images), batch_size=len(images), layer_to_target=layer_to_target)\n",
    "    \n",
    "    def get_clean_for_display(self):\n",
    "        return self._undo_preprocess(self.clean_images)\n",
    "    \n",
    "    def compute_clean_embeddings_with_model(self, model, layer_to_target):\n",
    "        self.clean_embeddings = self._compute_embeddings(model, self.clean_images, layer_to_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyModel:\n",
    "    def __init__(self, model_path, inputs=None, sess=None):\n",
    "        if inputs is None:\n",
    "            self.model_inputs = tf.placeholder(tf.float32, shape=(None, 160, 160, 3))\n",
    "        else:\n",
    "            self.model_inputs = inputs\n",
    "        \n",
    "        vars_before = tf.global_variables()\n",
    "        self.net, self.endpoints = inference(\n",
    "            self.model_inputs, keep_probability=1.0, bottleneck_layer_size=512, phase_train=False)\n",
    "        vars_after = tf.global_variables()\n",
    "        \n",
    "        model_name = dataset_to_model_name[\"vggface2\"]\n",
    "        saver = tf.train.Saver(list(set(vars_after) - set(vars_before)))\n",
    "        \n",
    "        if sess is None:\n",
    "            self.sess = tf.Session()\n",
    "        else:\n",
    "            self.sess = sess\n",
    "            \n",
    "        saver.restore(self.sess, model_path)\n",
    "    \n",
    "    def predict(self, inputs, batch_size, layer_to_target):\n",
    "        return self.sess.run(self.endpoints[layer_to_target], feed_dict={self.model_inputs: inputs})\n",
    "    \n",
    "    def __del__(self):\n",
    "        self.sess.close()\n",
    "\n",
    "dataset_to_model_name = {\n",
    "    \"casia-webface\": \"20180408-102900\",\n",
    "    \"vggface2\": \"20180402-114759\"\n",
    "}\n",
    "\n",
    "dataset_to_ckpt_number = {\n",
    "    \"casia-webface\": \"90\",\n",
    "    \"vggface2\": \"275\"\n",
    "}\n",
    "\n",
    "def build_model(dataset_name, inputs=None, sess=None):\n",
    "    model_name = dataset_to_model_name[dataset_name]\n",
    "    model = MyModel(\n",
    "        os.path.join(\n",
    "            \"/home/ivan/facenet/models\",\n",
    "            model_name,\n",
    "            \"model-{model_name}.ckpt-{ckpt_num}\".format(\n",
    "                model_name=dataset_to_model_name[dataset_name],\n",
    "                ckpt_num=dataset_to_ckpt_number[dataset_name]\n",
    "            )),\n",
    "        inputs, \n",
    "        sess\n",
    "    )\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def path_for_id_clean(identity):\n",
    "    if identity == \"n000958\":\n",
    "        return \"/data/vggface/test_perturbed_sampled/{id}/community_naive_mean/n000029/epsilon_0.0/png\".format(id=identity)\n",
    "    else:\n",
    "        return \"/data/vggface/test_perturbed_sampled/{id}/community_naive_mean/n000958/epsilon_0.0/png\".format(id=identity)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_person(person_name, path_for_id_fn, model=None, layer_to_target=None):\n",
    "    person_a = PersonImages(person_name)\n",
    "    person_a.load_clean_from_folder(path_for_id_fn(person_a.person_name))\n",
    "    if model is not None:\n",
    "        print(\"Computing embeddings for\", person_name)\n",
    "        person_a.compute_clean_embeddings_with_model(model, layer_to_target)\n",
    "    return person_a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Scale of 0 disables regularizer.\n",
      "INFO:tensorflow:Restoring parameters from /home/ivan/facenet/models/20180402-114759/model-20180402-114759.ckpt-275\n",
      "Computing embeddings for n001781\n",
      "Computing embeddings for n009232\n",
      "Computing embeddings for n000958\n",
      "Computing embeddings for n003356\n",
      "Computing embeddings for n008655\n",
      "Computing embeddings for n008613\n",
      "Computing embeddings for n004658\n",
      "Computing embeddings for n001683\n",
      "Computing embeddings for n002647\n",
      "Computing embeddings for n009288\n",
      "Computing embeddings for n005427\n",
      "Computing embeddings for n002763\n",
      "Computing embeddings for n002503\n",
      "Computing embeddings for n003215\n",
      "Computing embeddings for n005359\n",
      "Computing embeddings for n005303\n",
      "Computing embeddings for n007548\n",
      "Computing embeddings for n000029\n",
      "Computing embeddings for n009114\n"
     ]
    }
   ],
   "source": [
    "vggface2_model = build_model(\"vggface2\")\n",
    "people = [\n",
    "    build_person(person_name, path_for_id_clean, vggface2_model, layer_to_target) \\\n",
    "    for person_name in os.listdir(\"/data/vggface/test_perturbed_sampled\")\n",
    "]\n",
    "del vggface2_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_vggface_model(inputs, sess):\n",
    "    return build_model(\"vggface2\", inputs, sess)\n",
    "\n",
    "def build_casiawebface_model(inputs, sess):\n",
    "    return build_model(\"casia-webface\", inputs, sess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def attack_batch_cw(\n",
    "    build_model_fn, \n",
    "    input_images,\n",
    "    target_vectors,\n",
    "    learning_rate,\n",
    "    epsilon,\n",
    "    max_iters,\n",
    "    layer_to_target\n",
    "):\n",
    "    input_images = np.array(input_images)\n",
    "    batch_size, orig_h, orig_w, orig_c = input_images.shape\n",
    "    print(\"input images mean {mean} and stddev {stddev}\".format(mean=np.mean(input_images), stddev=np.std(input_images)))\n",
    "    with tf.Graph().as_default():\n",
    "        with tf.Session() as sess:\n",
    "            targets_var = tf.get_variable(\n",
    "                name=\"targets\",\n",
    "                shape=target_vectors.shape,\n",
    "            )\n",
    "            sess.run(tf.assign(targets_var, target_vectors))\n",
    "            \n",
    "            images_input_ph = tf.placeholder(\n",
    "                tf.float32,\n",
    "                name=\"input_images\",\n",
    "                shape=input_images.shape\n",
    "            )\n",
    "            \n",
    "            noise_var = tf.get_variable(\n",
    "                name=\"adversarial_noise\",\n",
    "                shape=input_images.shape,\n",
    "                initializer=tf.initializers.truncated_normal(\n",
    "                    mean=np.mean(input_images),\n",
    "                    stddev=3 * np.std(input_images)\n",
    "                )\n",
    "            )\n",
    "            sess.run(noise_var.initializer)\n",
    "            \n",
    "            images_plus_noise = images_input_ph + noise_var\n",
    "\n",
    "            randomized_images_plus_noise = tf.image.random_brightness(\n",
    "                images_plus_noise, 0.5)\n",
    "        \n",
    "            randomized_images_plus_noise = tf.image.random_crop(\n",
    "                randomized_images_plus_noise, \n",
    "                [batch_size, orig_h - 10, orig_w - 10, 3]\n",
    "            )\n",
    "\n",
    "            randomized_images_plus_noise = tf.image.resize_images(\n",
    "                randomized_images_plus_noise, [orig_h, orig_w])\n",
    "            \n",
    "            randomized_images_plus_noise += tf.random.normal(\n",
    "                randomized_images_plus_noise.shape, 0.0, 0.75)\n",
    "            \n",
    "            randomized_images_plus_noise = tf.clip_by_value(\n",
    "                randomized_images_plus_noise, input_images - epsilon, input_images + epsilon)\n",
    "            \n",
    "            model = build_model_fn(\n",
    "                inputs=randomized_images_plus_noise, \n",
    "                sess=sess\n",
    "            )\n",
    "            \n",
    "            model_outputs = model.endpoints[layer_to_target]\n",
    "            assert np.array_equal(model_outputs.shape.as_list(), targets_var.shape.as_list()), \"Shape mismatch\"\n",
    "            loss = tf.reduce_mean((model_outputs - targets_var) ** 2)\n",
    "            \n",
    "        \n",
    "            total_loss = loss + 1e-6 * tf.nn.l2_loss(noise_var)\n",
    "            \n",
    "            vars_before = set(tf.global_variables())\n",
    "            train_op = tf.train.AdamOptimizer(learning_rate).minimize(\n",
    "                total_loss, var_list=[noise_var])\n",
    "            vars_after = set(tf.global_variables())\n",
    "            sess.run([v.initializer for v in list(vars_after - vars_before)])\n",
    "            \n",
    "            \n",
    "            losses = []\n",
    "            for i in range(max_iters):\n",
    "                loss_value, total_loss_value, _ = sess.run(\n",
    "                    [loss, total_loss, train_op], feed_dict={images_input_ph: input_images})\n",
    "                assert not np.isnan(loss_value), \"Loss_value is nan\"\n",
    "                losses.append(loss_value)\n",
    "            \n",
    "            final_imgs = sess.run(\n",
    "                tf.clip_by_value(images_plus_noise, input_images - epsilon, input_images + epsilon),\n",
    "                feed_dict={images_input_ph: input_images}\n",
    "            )\n",
    "            \n",
    "            return final_imgs, losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_decoys_bigger_batches(\n",
    "    people_list,\n",
    "    attack_strategy,\n",
    "    model_to_attack,\n",
    "    learning_rate,\n",
    "    epsilon,\n",
    "    max_iters,\n",
    "    layer_to_target\n",
    "):\n",
    "    if model_to_attack == \"vggface2\":\n",
    "        model_build_fn = build_vggface_model\n",
    "    elif model_to_attack == \"casia-webface\":\n",
    "        model_build_fn = build_casiawebface_model\n",
    "        \n",
    "    for person in tqdm(people_list):\n",
    "        num, height, width, channels = person.clean_images.shape\n",
    "        person.adversarial_images = np.zeros((2*(len(people_list) - 1), height, width, channels))\n",
    "        \n",
    "        indx = 0\n",
    "        \n",
    "        images_to_make_adversarial = []\n",
    "        targets_for_images = []\n",
    "        \n",
    "        for other_person in people_list:\n",
    "            if person.person_name == other_person.person_name:\n",
    "                continue\n",
    "                \n",
    "            current_chosen_indices = range(indx, indx + 2)\n",
    "            images_to_make_adversarial.extend(np.take(\n",
    "                person.clean_images, current_chosen_indices, axis=0))\n",
    "            \n",
    "            target_vector = np.mean(other_person.clean_embeddings, axis=0)\n",
    "            targets_for_images.extend(np.array([\n",
    "                target_vector for _ in range(len(current_chosen_indices))]))\n",
    "            \n",
    "            indx += 2\n",
    "            \n",
    "        images_to_make_adversarial = np.array(images_to_make_adversarial)\n",
    "        \n",
    "        all_adversarial_images, losses = attack_batch_cw(\n",
    "            model_build_fn, \n",
    "            np.array(images_to_make_adversarial),\n",
    "            np.array(targets_for_images),\n",
    "            learning_rate,\n",
    "            epsilon,\n",
    "            max_iters,\n",
    "            layer_to_target\n",
    "        )\n",
    "            \n",
    "        fig, ax = plt.subplots()\n",
    "        ax.plot(range(len(losses)), losses)\n",
    "        plt.show()\n",
    "        \n",
    "        indx = 0\n",
    "        for other_person in people_list:\n",
    "            if person.person_name == other_person.person_name:\n",
    "                continue\n",
    "            \n",
    "            save_dest = os.path.join(\n",
    "                \"/data/vggface/test_perturbed_sampled\",\n",
    "                person.person_name,\n",
    "                \"{attack_strategy}_{model}\".format(attack_strategy=attack_strategy, model=model_to_attack),\n",
    "                other_person.person_name\n",
    "            )\n",
    "            \n",
    "            save_path = os.path.join(save_dest, \"epsilon_{}\".format(epsilon), \"png\")\n",
    "            os.makedirs(save_path, exist_ok=True)\n",
    "            existing_files = os.listdir(save_path)\n",
    "\n",
    "            # Clean up folder if need be\n",
    "            if len(existing_files) > 0:\n",
    "                for f in existing_files:\n",
    "                    os.remove(os.path.join(save_path, f))\n",
    "            \n",
    "            for i in range(indx, indx + 2):\n",
    "                orig_name = person.orig_paths[i].split(\"/\")[-1]\n",
    "                person.adversarial_images[i] = all_adversarial_images[i]\n",
    "                full_save_path = os.path.join(save_path, orig_name)\n",
    "                print(\"Saving to\", save_path)\n",
    "                Image.fromarray(\n",
    "                    person._undo_preprocess(person.adversarial_images[i])\n",
    "                ).save(full_save_path)\n",
    "\n",
    "            indx += 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/19 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input images mean 9.97825910786787e-09 and stddev 1.0000001192092896\n",
      "INFO:tensorflow:Scale of 0 disables regularizer.\n",
      "INFO:tensorflow:Restoring parameters from /home/ivan/facenet/models/20180402-114759/model-20180402-114759.ckpt-275\n"
     ]
    }
   ],
   "source": [
    "for epsilon in [0.1, 0.25, 0.5]:\n",
    "    generate_decoys_bigger_batches(\n",
    "        people,\n",
    "        \"mean_{}\".format(layer_to_target),\n",
    "        \"vggface2\",\n",
    "        0.01,\n",
    "        epsilon,\n",
    "        2000,\n",
    "        layer_to_target\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epsilon in [0.1, 0.25, 0.5]:\n",
    "    generate_decoys_bigger_batches(\n",
    "        people,\n",
    "        \"mean_{}\".format(layer_to_target),\n",
    "        \"casia-webface\",\n",
    "        0.01,\n",
    "        epsilon,\n",
    "        2000,\n",
    "        layer_to_target\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FacialVectorsDatabase:\n",
    "    def __init__(self, folder, model_name, epsilon, attack_strategy, image_format, num_clean, include_decoys=True):\n",
    "        self.associated_identities = []\n",
    "        self.vectors = []\n",
    "        self.associated_paths = []\n",
    "        \n",
    "        identities = os.listdir(folder)\n",
    "        \n",
    "        \n",
    "        self.graph = tf.Graph()\n",
    "        \n",
    "        with self.graph.as_default():\n",
    "            self.model = build_model(model_name)\n",
    "\n",
    "            for person_name in tqdm(identities):\n",
    "                attack_subfolder = os.path.join(folder, person_name, attack_strategy)\n",
    "\n",
    "                protected = os.listdir(attack_subfolder)\n",
    "                \n",
    "                if include_decoys:\n",
    "                    for indx, other_identity in enumerate(protected):\n",
    "                        protected_folder = os.path.join(\n",
    "                            attack_subfolder, \n",
    "                            other_identity, \n",
    "                            \"epsilon_{eps}\".format(eps=epsilon), \n",
    "                            image_format\n",
    "                        )\n",
    "\n",
    "                        self._add_folder_for_person(\n",
    "                            protected_folder, \n",
    "                            person_name,\n",
    "                            exclude_endings=None,\n",
    "                            max_imgs=-1\n",
    "                        )\n",
    "                        \n",
    "                # These were the images we used for this person for adversarial modification\n",
    "                used_images = [\n",
    "                    x.split(\"/\")[-1] \\\n",
    "                    for indx, x in enumerate(self.associated_paths) \\\n",
    "                    if self.associated_identities[indx] == person_name\n",
    "                ]\n",
    "                \n",
    "                clean_folder = os.path.join(\n",
    "                    folder, person_name, \"community_naive_mean\", protected[0], \"epsilon_0.0\", \"png\")\n",
    "                self._add_folder_for_person(\n",
    "                    clean_folder, \n",
    "                    person_name,\n",
    "                    exclude_endings=set(used_images),\n",
    "                    max_imgs=num_clean\n",
    "                )\n",
    "        \n",
    "                \n",
    "    def _load_one_facenet(self, path, crop_box=None, resize_size=None, prewhiten_img=True):\n",
    "        img = Image.open(path).convert(\"RGB\")\n",
    "        \n",
    "        if crop_box:\n",
    "            img = img.crop(crop_box)\n",
    "            \n",
    "        if resize_size:\n",
    "            img = img.resize((resize_size, resize_size))\n",
    "            \n",
    "        img = (np.array(img)).astype(np.float32)\n",
    "        \n",
    "        if prewhiten_img:\n",
    "            img = prewhiten(img)\n",
    "            \n",
    "        return img\n",
    "        \n",
    "\n",
    "    def _load_folder_for_facenet(self, folder, exclude_endings=None, max_imgs=-1):\n",
    "        paths_list = glob.glob(os.path.join(folder, \"*\"))\n",
    "        len_before = len(paths_list)\n",
    "        if not (exclude_endings is None):\n",
    "            paths_list = [x for x in paths_list if not (x.split(\"/\")[-1] in exclude_endings)]\n",
    "            \n",
    "        if max_imgs > 0:\n",
    "            paths_list = paths_list[:max_imgs]\n",
    "        \n",
    "        final_imgs = [\n",
    "            self._load_one_facenet(\n",
    "                img_path, \n",
    "                prewhiten_img=False, \n",
    "                resize_size=None, \n",
    "                crop_box=None) for img_path in paths_list\n",
    "        ]\n",
    "\n",
    "        final_imgs = np.array(final_imgs)\n",
    "        final_imgs = prewhiten(np.array(final_imgs))\n",
    "        return final_imgs, paths_list\n",
    "\n",
    "    \n",
    "    def _compute_embeddings(self, images):\n",
    "        return self.model.predict(np.array(images), batch_size=len(images))\n",
    "    \n",
    "    \n",
    "    def _add_folder_for_person(self, folder, person_name, exclude_endings=None, max_imgs=-1):\n",
    "        images, paths_list = self._load_folder_for_facenet(folder, exclude_endings, max_imgs=max_imgs)\n",
    "        \n",
    "        if images is None:\n",
    "            return\n",
    "        \n",
    "        vectors = self._compute_embeddings(images)\n",
    "        self.vectors.extend(vectors)\n",
    "        self.associated_identities.extend([person_name for _ in range(len(vectors))])\n",
    "        self.associated_paths.extend(paths_list)\n",
    "    \n",
    "    def nearest_neighbor_to_img_at_path(self, query_path):\n",
    "        with self.graph.as_default():\n",
    "            query_vector = self._compute_embeddings(\n",
    "                np.expand_dims(self._load_one_facenet(query_path, resize_size=160), axis=0))\n",
    "        distances = pairwise_distances(query_vector, self.vectors, metric=\"cosine\")[0]\n",
    "        min_dist_indx = np.argmin(distances)\n",
    "        return self.associated_identities[min_dist_indx], self.associated_paths[min_dist_indx]\n",
    "    \n",
    "\n",
    "def full_path_to_info(path):\n",
    "    split = path.split(\"/\")\n",
    "    if path.startswith(\"/data/vggface/test_perturbed_sampled\"):\n",
    "        # split of path breaks down like this:\n",
    "        #     0 \n",
    "        #     1 data\n",
    "        #     2 vggface\n",
    "        #     3 test_perturbed_sampled\n",
    "        #     4 {protector}\n",
    "        #     5 community_naive_mean\n",
    "        #     6 {protected}\n",
    "        #     7 epsilon_0.0\n",
    "        #     8 png\n",
    "        #     9 35.png\n",
    "        return \"-\".join([split[4], split[6], split[7], split[9]])\n",
    "    else:\n",
    "        # split of path breaks down like this:\n",
    "        #     0 \n",
    "        #     1 data\n",
    "        #     2 vggface\n",
    "        #     3 test_query_antisampled\n",
    "        #     4 {protector}\n",
    "        #     5 image_name.jpeg\n",
    "        return \"-\".join([split[4], split[5]])\n",
    "    \n",
    "    \n",
    "def measure_local_recall(\n",
    "    faces_database,\n",
    "    image_directory=\"/data/vggface/test_query_antisampled\",\n",
    "    num_query=10,\n",
    "    verbose=False\n",
    "):\n",
    "    discovery = []\n",
    "    true = []\n",
    "    identified_as = []\n",
    "    \n",
    "    paths_of_query = []\n",
    "    paths_of_nearest = []\n",
    "\n",
    "    for protector in os.listdir(image_directory):\n",
    "        # We are sourcing query photos from epsilon_0.0.\n",
    "        # In those cases, all subfolders in the \"protected\" identity have the same, clean\n",
    "        # photo of the protector, so we just pick any single one that exists (e.g. n000958)\n",
    "        # For the case where n000958 is itself the protector, n000958 is not present in its protected\n",
    "        # subfolders, so we pick n000029 without loss of generality.\n",
    "        if protector == \"n000958\":\n",
    "            protected = \"n000029\"\n",
    "        else:\n",
    "            protected = \"n000958\"\n",
    "\n",
    "        query_photos_paths = sorted(glob.glob(\n",
    "            os.path.join(image_directory, protector, \"*\")\n",
    "        ))\n",
    "\n",
    "        \n",
    "        for i in np.random.choice(len(query_photos_paths), num_query):\n",
    "            chosen_path = query_photos_paths[i]\n",
    "            top_identity, top_identity_path = faces_database.nearest_neighbor_to_img_at_path(chosen_path)\n",
    "            \n",
    "            paths_of_query.append(full_path_to_info(chosen_path))\n",
    "            paths_of_nearest.append(full_path_to_info(top_identity_path))\n",
    "            \n",
    "            true.append(protector)\n",
    "            identified_as.append(top_identity)\n",
    "\n",
    "            if top_identity == protector:\n",
    "                discovery.append(1.0)\n",
    "            else:\n",
    "                discovery.append(0.0)\n",
    "\n",
    "    if verbose:\n",
    "        for true_id, recognized_id, query, nearest in zip(true, identified_as, paths_of_query, paths_of_nearest):\n",
    "            print(\"Face of {true_id} identitifed as {recognized_id}. Nearest neighbor to {query} was {nearest}\".format(\n",
    "                true_id=true_id, recognized_id=recognized_id, query=query, nearest=nearest))\n",
    "        \n",
    "    return sum(discovery)/len(discovery)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_recall_once(\n",
    "    network_to_evaluate,\n",
    "    epsilon,\n",
    "    attack_name,\n",
    "    num_clean,\n",
    "    include_adversarial,\n",
    "    run\n",
    "):\n",
    "    faces_db = FacialVectorsDatabase(\n",
    "        folder=\"/data/vggface/test_perturbed_sampled\",\n",
    "        model_name=network_to_evaluate,\n",
    "        epsilon=epsilon,\n",
    "        attack_strategy=attack_name,\n",
    "        image_format=\"png\", \n",
    "        num_clean=num_clean,\n",
    "        include_decoys=include_adversarial\n",
    "    )\n",
    "    recall_at_k1 = measure_local_recall(faces_db, num_query=10, verbose=False)\n",
    "\n",
    "    faces_db.model.sess.close()\n",
    "    \n",
    "    return {\n",
    "        \"num_clean\": num_clean, \n",
    "        \"run\": run,\n",
    "        \"recall\": recall_at_k1,\n",
    "        \"epsilon\": epsilon\n",
    "    }\n",
    "\n",
    "def generate_results_plot(\n",
    "    network_to_evaluate,\n",
    "    attack_name\n",
    "):\n",
    "    results = []\n",
    "    \n",
    "    # get a run with all clean ones and no decous\n",
    "    res = get_recall_once(\n",
    "        network_to_evaluate,\n",
    "        0.5, # epsilon is irrelevant\n",
    "        attack_name,\n",
    "        -1, # negative or 0 number includes all\n",
    "        False,\n",
    "        11\n",
    "    )\n",
    "    results.append(res)\n",
    "    \n",
    "    for epsilon in [0.1, 0.25, 0.5]:\n",
    "        for num_clean in range(1, 10, 2):\n",
    "            for run in range(10):\n",
    "                res = get_recall_once(\n",
    "                    network_to_evaluate,\n",
    "                    epsilon,\n",
    "                    attack_name,\n",
    "                    num_clean,\n",
    "                    True,\n",
    "                    run\n",
    "                )\n",
    "                results.append(res)\n",
    "                \n",
    "    results_pd = pd.DataFrame(results)\n",
    "\n",
    "    results_pd[\"decoys_vs_clean\"] = 36 / results_pd[\"num_clean\"]\n",
    "    results_pd[results_pd.decoys_vs_clean < 0.0][\"decoys_vs_clean\"] = 0.0\n",
    "    \n",
    "    results_pd.to_csv(\n",
    "        \"/home/ivan/pascal_adversarial_faces/results/recall_at_1_{attack_name}_{network_to_evaluate}.csv\".format(\n",
    "            network_to_evaluate=network_to_evaluate,\n",
    "            attack_name=attack_name\n",
    "        ))\n",
    "\n",
    "    ax = sns.lineplot(\n",
    "        data=results_pd, x=\"decoys_vs_clean\", y=\"recall\", style=\"epsilon\", markers=True)\n",
    "    ax.set_title(\"Attack {attack} evaluated on {network_to_evaluate}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_results_plot(\n",
    "    \"casia-webface\",\n",
    "    \"mean_{}_vggface2\".format(layer_to_target)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_results_plot(\n",
    "    \"vggface2\",\n",
    "    \"mean_{}_casia-webface\".format(layer_to_target)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_results_plot(\n",
    "    \"vggface2\",\n",
    "    \"mean_{}_vggface2\".format(layer_to_target)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_results_plot(\n",
    "    \"casia-webface\",\n",
    "    \"mean_{}_casia-webface\".format(layer_to_target)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
