{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "import tensorflow as tf\n",
    "import glob\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from sklearn.metrics import pairwise_distances, roc_auc_score\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline\n",
    "from tqdm import tqdm\n",
    "import sys\n",
    "sys.path.append(\"../\")\n",
    "from inception_resnet_v1 import inference\n",
    "import pandas as pd\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer_to_target = \"Conv2d_4b_3x3\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prewhiten(x):\n",
    "    \"\"\"\n",
    "    A helper function to whiten an image, or a batch of images.\n",
    "    Args:\n",
    "        x: An image or batch of images.\n",
    "    \"\"\"\n",
    "    if x.ndim == 4:\n",
    "        axis = (1, 2, 3)\n",
    "        size = x[0].size\n",
    "    elif x.ndim == 3:\n",
    "        axis = (0, 1, 2)\n",
    "        size = x.size\n",
    "    else:\n",
    "        print(x.ndim)\n",
    "        raise ValueError('Dimension should be 3 or 4')\n",
    "\n",
    "    mean = np.mean(x, axis=axis, keepdims=True)\n",
    "    std = np.std(x, axis=axis, keepdims=True)\n",
    "#     std_adj = np.maximum(std, 1.0/np.sqrt(size))\n",
    "    y = (x - mean) / std\n",
    "    return y\n",
    "\n",
    "def l2_normalize(x, axis=-1, epsilon=1e-10):\n",
    "    \"\"\"\n",
    "    Normalizes an embedding to have unit length in the l2 metric.\n",
    "    Args:\n",
    "        x: A batch of numpy embeddings\n",
    "    \"\"\"\n",
    "    output = x / np.sqrt(np.maximum(np.sum(np.square(x),\n",
    "                                           axis=axis,\n",
    "                                           keepdims=True),\n",
    "                                    epsilon))\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PersonImages:\n",
    "    def __init__(self, person_name):\n",
    "        self.clean_folder = \"\"\n",
    "        self.clean_images = []\n",
    "        self.adversarial_images = []\n",
    "        self.orig_mean = None\n",
    "        self.orig_std = None\n",
    "        self.orig_paths = []\n",
    "        self.person_name = person_name\n",
    "    \n",
    "    def _load_one_facenet(self, path, resize_size=None, prewhiten_img=True):\n",
    "        img = Image.open(path).convert(\"RGB\")\n",
    "    \n",
    "            \n",
    "        if resize_size:\n",
    "            img = img.resize((resize_size, resize_size))\n",
    "            \n",
    "        img = (np.array(img)).astype(np.float32)\n",
    "        \n",
    "        if prewhiten_img:\n",
    "            img = prewhiten(img)\n",
    "            \n",
    "        return img\n",
    "        \n",
    "    def _load_folder_for_facenet(self, folder, resize_size=None):\n",
    "        paths_list = glob.glob(os.path.join(folder, \"*\"))\n",
    "        final_imgs = []\n",
    "        for img_path in paths_list:\n",
    "            final_imgs.append(\n",
    "                self._load_one_facenet(\n",
    "                    img_path, prewhiten_img=False, resize_size=resize_size))\n",
    "        \n",
    "        final_imgs = np.array(final_imgs)\n",
    "        mean, std = np.mean(final_imgs), np.std(final_imgs)\n",
    "        final_imgs = prewhiten(np.array(final_imgs))\n",
    "        return final_imgs, mean, std, paths_list\n",
    "        \n",
    "    def load_clean_from_folder(self, clean_folder, resize_size=160):\n",
    "        self.clean_folder = clean_folder\n",
    "        self.clean_images, self.orig_mean, self.orig_std, self.orig_paths = self._load_folder_for_facenet(\n",
    "            clean_folder, resize_size=resize_size)\n",
    "    \n",
    "    def _undo_preprocess(self, images):\n",
    "        restored_images = images.copy()\n",
    "        restored_images  *= self.orig_std\n",
    "        restored_images += self.orig_mean\n",
    "        restored_images = np.clip(restored_images, 0.0, 255.0)\n",
    "        return np.uint8(restored_images)\n",
    "    \n",
    "    def _compute_embeddings(self, model, images, layer_to_target):\n",
    "        return model.predict(np.array(images), batch_size=len(images), layer_to_target=layer_to_target)\n",
    "    \n",
    "    def get_clean_for_display(self):\n",
    "        return self._undo_preprocess(self.clean_images)\n",
    "    \n",
    "    def compute_clean_embeddings_with_model(self, model, layer_to_target):\n",
    "        self.clean_embeddings = self._compute_embeddings(model, self.clean_images, layer_to_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyModel:\n",
    "    def __init__(self, model_path, inputs=None, sess=None):\n",
    "        if inputs is None:\n",
    "            self.model_inputs = tf.placeholder(tf.float32, shape=(None, 160, 160, 3))\n",
    "        else:\n",
    "            self.model_inputs = inputs\n",
    "        \n",
    "        vars_before = tf.global_variables()\n",
    "        self.net, self.endpoints = inference(\n",
    "            self.model_inputs, keep_probability=1.0, bottleneck_layer_size=512, phase_train=False)\n",
    "        vars_after = tf.global_variables()\n",
    "        \n",
    "        model_name = dataset_to_model_name[\"vggface2\"]\n",
    "        saver = tf.train.Saver(list(set(vars_after) - set(vars_before)))\n",
    "        \n",
    "        if sess is None:\n",
    "            self.sess = tf.Session()\n",
    "        else:\n",
    "            self.sess = sess\n",
    "            \n",
    "        saver.restore(self.sess, model_path)\n",
    "    \n",
    "    def predict(self, inputs, batch_size, layer_to_target):\n",
    "        return self.sess.run(self.endpoints[layer_to_target], feed_dict={self.model_inputs: inputs})\n",
    "    \n",
    "    def __del__(self):\n",
    "        self.sess.close()\n",
    "\n",
    "dataset_to_model_name = {\n",
    "    \"casia-webface\": \"20180408-102900\",\n",
    "    \"vggface2\": \"20180402-114759\"\n",
    "}\n",
    "\n",
    "dataset_to_ckpt_number = {\n",
    "    \"casia-webface\": \"90\",\n",
    "    \"vggface2\": \"275\"\n",
    "}\n",
    "\n",
    "def build_model(dataset_name, inputs=None, sess=None):\n",
    "    model_name = dataset_to_model_name[dataset_name]\n",
    "    model = MyModel(\n",
    "        os.path.join(\n",
    "            \"/home/ivan/facenet/models\",\n",
    "            model_name,\n",
    "            \"model-{model_name}.ckpt-{ckpt_num}\".format(\n",
    "                model_name=dataset_to_model_name[dataset_name],\n",
    "                ckpt_num=dataset_to_ckpt_number[dataset_name]\n",
    "            )),\n",
    "        inputs, \n",
    "        sess\n",
    "    )\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def path_for_id_clean(identity):\n",
    "    if identity == \"n000958\":\n",
    "        return \"/data/vggface/test_perturbed_sampled/{id}/community_naive_mean/n000029/epsilon_0.0/png\".format(id=identity)\n",
    "    else:\n",
    "        return \"/data/vggface/test_perturbed_sampled/{id}/community_naive_mean/n000958/epsilon_0.0/png\".format(id=identity)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_person(person_name, path_for_id_fn, model=None, layer_to_target=None):\n",
    "    person_a = PersonImages(person_name)\n",
    "    person_a.load_clean_from_folder(path_for_id_fn(person_a.person_name))\n",
    "    if model is not None:\n",
    "        print(\"Computing embeddings for\", person_name)\n",
    "        person_a.compute_clean_embeddings_with_model(model, layer_to_target)\n",
    "    return person_a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Scale of 0 disables regularizer.\n",
      "INFO:tensorflow:Restoring parameters from /home/ivan/facenet/models/20180402-114759/model-20180402-114759.ckpt-275\n",
      "Computing embeddings for n001781\n",
      "Computing embeddings for n009232\n",
      "Computing embeddings for n000958\n",
      "Computing embeddings for n003356\n",
      "Computing embeddings for n008655\n",
      "Computing embeddings for n008613\n",
      "Computing embeddings for n004658\n",
      "Computing embeddings for n001683\n",
      "Computing embeddings for n002647\n",
      "Computing embeddings for n009288\n",
      "Computing embeddings for n005427\n",
      "Computing embeddings for n002763\n",
      "Computing embeddings for n002503\n",
      "Computing embeddings for n003215\n",
      "Computing embeddings for n005359\n",
      "Computing embeddings for n005303\n",
      "Computing embeddings for n007548\n",
      "Computing embeddings for n000029\n",
      "Computing embeddings for n009114\n"
     ]
    }
   ],
   "source": [
    "vggface2_model = build_model(\"vggface2\")\n",
    "people = [\n",
    "    build_person(person_name, path_for_id_clean, vggface2_model, layer_to_target) \\\n",
    "    for person_name in os.listdir(\"/data/vggface/test_perturbed_sampled\")\n",
    "]\n",
    "del vggface2_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_vggface_model(inputs, sess):\n",
    "    return build_model(\"vggface2\", inputs, sess)\n",
    "\n",
    "def build_casiawebface_model(inputs, sess):\n",
    "    return build_model(\"casia-webface\", inputs, sess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def attack_batch_cw(\n",
    "    build_model_fn, \n",
    "    input_images,\n",
    "    target_vectors,\n",
    "    learning_rate,\n",
    "    epsilon,\n",
    "    max_iters,\n",
    "    layer_to_target\n",
    "):\n",
    "    input_images = np.array(input_images)\n",
    "    batch_size, orig_h, orig_w, orig_c = input_images.shape\n",
    "    print(\"input images mean {mean} and stddev {stddev}\".format(mean=np.mean(input_images), stddev=np.std(input_images)))\n",
    "    with tf.Graph().as_default():\n",
    "        with tf.Session() as sess:\n",
    "            targets_var = tf.get_variable(\n",
    "                name=\"targets\",\n",
    "                shape=target_vectors.shape,\n",
    "            )\n",
    "            sess.run(tf.assign(targets_var, target_vectors))\n",
    "            \n",
    "            images_input_ph = tf.placeholder(\n",
    "                tf.float32,\n",
    "                name=\"input_images\",\n",
    "                shape=input_images.shape\n",
    "            )\n",
    "            \n",
    "            noise_var = tf.get_variable(\n",
    "                name=\"adversarial_noise\",\n",
    "                shape=input_images.shape,\n",
    "                initializer=tf.initializers.truncated_normal(\n",
    "                    mean=np.mean(input_images),\n",
    "                    stddev=3 * np.std(input_images)\n",
    "                )\n",
    "            )\n",
    "            sess.run(noise_var.initializer)\n",
    "            \n",
    "            images_plus_noise = images_input_ph + noise_var\n",
    "\n",
    "            randomized_images_plus_noise = tf.image.random_brightness(\n",
    "                images_plus_noise, 0.5)\n",
    "        \n",
    "            randomized_images_plus_noise = tf.image.random_crop(\n",
    "                randomized_images_plus_noise, \n",
    "                [batch_size, orig_h - 10, orig_w - 10, 3]\n",
    "            )\n",
    "\n",
    "            randomized_images_plus_noise = tf.image.resize_images(\n",
    "                randomized_images_plus_noise, [orig_h, orig_w])\n",
    "            \n",
    "            randomized_images_plus_noise += tf.random.normal(\n",
    "                randomized_images_plus_noise.shape, 0.0, 0.75)\n",
    "            \n",
    "            randomized_images_plus_noise = tf.clip_by_value(\n",
    "                randomized_images_plus_noise, input_images - epsilon, input_images + epsilon)\n",
    "            \n",
    "            model = build_model_fn(\n",
    "                inputs=randomized_images_plus_noise, \n",
    "                sess=sess\n",
    "            )\n",
    "            \n",
    "            model_outputs = model.endpoints[layer_to_target]\n",
    "            assert np.array_equal(model_outputs.shape.as_list(), targets_var.shape.as_list()), \"Shape mismatch\"\n",
    "            loss = tf.nn.l2_loss(model_outputs - targets_var)\n",
    "            \n",
    "        \n",
    "            total_loss = loss + 1e-6 * tf.nn.l2_loss(noise_var)\n",
    "            \n",
    "            vars_before = set(tf.global_variables())\n",
    "            train_op = tf.train.AdamOptimizer(learning_rate).minimize(\n",
    "                total_loss, var_list=[noise_var])\n",
    "            vars_after = set(tf.global_variables())\n",
    "            sess.run([v.initializer for v in list(vars_after - vars_before)])\n",
    "            \n",
    "            \n",
    "            losses = []\n",
    "            for i in range(max_iters):\n",
    "                loss_value, total_loss_value, _ = sess.run(\n",
    "                    [loss, total_loss, train_op], feed_dict={images_input_ph: input_images})\n",
    "                assert not np.isnan(loss_value), \"Loss_value is nan\"\n",
    "                losses.append(loss_value)\n",
    "            \n",
    "            final_imgs = sess.run(\n",
    "                tf.clip_by_value(images_plus_noise, input_images - epsilon, input_images + epsilon),\n",
    "                feed_dict={images_input_ph: input_images}\n",
    "            )\n",
    "            \n",
    "            return final_imgs, losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_decoys_bigger_batches(\n",
    "    people_list,\n",
    "    attack_strategy,\n",
    "    model_to_attack,\n",
    "    learning_rate,\n",
    "    epsilon,\n",
    "    max_iters,\n",
    "    layer_to_target\n",
    "):\n",
    "    if model_to_attack == \"vggface2\":\n",
    "        model_build_fn = build_vggface_model\n",
    "    elif model_to_attack == \"casia-webface\":\n",
    "        model_build_fn = build_casiawebface_model\n",
    "        \n",
    "    for person in tqdm(people_list):\n",
    "        num, height, width, channels = person.clean_images.shape\n",
    "        person.adversarial_images = np.zeros((2*(len(people_list) - 1), height, width, channels))\n",
    "        \n",
    "        indx = 0\n",
    "        \n",
    "        images_to_make_adversarial = []\n",
    "        targets_for_images = []\n",
    "        \n",
    "        for other_person in people_list:\n",
    "            if person.person_name == other_person.person_name:\n",
    "                continue\n",
    "                \n",
    "            current_chosen_indices = range(indx, indx + 2)\n",
    "            images_to_make_adversarial.extend(np.take(\n",
    "                person.clean_images, current_chosen_indices, axis=0))\n",
    "            \n",
    "            target_vector = np.mean(other_person.clean_embeddings, axis=0)\n",
    "            targets_for_images.extend(np.array([\n",
    "                target_vector for _ in range(len(current_chosen_indices))]))\n",
    "            \n",
    "            indx += 2\n",
    "            \n",
    "        images_to_make_adversarial = np.array(images_to_make_adversarial)\n",
    "        \n",
    "        all_adversarial_images, losses = attack_batch_cw(\n",
    "            model_build_fn, \n",
    "            np.array(images_to_make_adversarial),\n",
    "            np.array(targets_for_images),\n",
    "            learning_rate,\n",
    "            epsilon,\n",
    "            max_iters,\n",
    "            layer_to_target\n",
    "        )\n",
    "            \n",
    "        fig, ax = plt.subplots()\n",
    "        ax.plot(range(len(losses)), losses)\n",
    "        plt.show()\n",
    "        \n",
    "        indx = 0\n",
    "        for other_person in people_list:\n",
    "            if person.person_name == other_person.person_name:\n",
    "                continue\n",
    "            \n",
    "            save_dest = os.path.join(\n",
    "                \"/data/vggface/test_perturbed_sampled\",\n",
    "                person.person_name,\n",
    "                \"{attack_strategy}_{model}\".format(attack_strategy=attack_strategy, model=model_to_attack),\n",
    "                other_person.person_name\n",
    "            )\n",
    "            \n",
    "            save_path = os.path.join(save_dest, \"epsilon_{}\".format(epsilon), \"png\")\n",
    "            os.makedirs(save_path, exist_ok=True)\n",
    "            existing_files = os.listdir(save_path)\n",
    "\n",
    "            # Clean up folder if need be\n",
    "            if len(existing_files) > 0:\n",
    "                for f in existing_files:\n",
    "                    os.remove(os.path.join(save_path, f))\n",
    "            \n",
    "            for i in range(indx, indx + 2):\n",
    "                orig_name = person.orig_paths[i].split(\"/\")[-1]\n",
    "                person.adversarial_images[i] = all_adversarial_images[i]\n",
    "                full_save_path = os.path.join(save_path, orig_name)\n",
    "                print(\"Saving to\", save_path)\n",
    "                Image.fromarray(\n",
    "                    person._undo_preprocess(person.adversarial_images[i])\n",
    "                ).save(full_save_path)\n",
    "\n",
    "            indx += 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/19 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input images mean 9.97825910786787e-09 and stddev 1.0000001192092896\n",
      "INFO:tensorflow:Scale of 0 disables regularizer.\n",
      "INFO:tensorflow:Restoring parameters from /home/ivan/facenet/models/20180402-114759/model-20180402-114759.ckpt-275\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving to /data/vggface/test_perturbed_sampled/n001781/mean_Conv2d_4b_3x3_vggface2/n009232/epsilon_0.1/png\n",
      "Saving to /data/vggface/test_perturbed_sampled/n001781/mean_Conv2d_4b_3x3_vggface2/n009232/epsilon_0.1/png\n",
      "Saving to /data/vggface/test_perturbed_sampled/n001781/mean_Conv2d_4b_3x3_vggface2/n000958/epsilon_0.1/png\n",
      "Saving to /data/vggface/test_perturbed_sampled/n001781/mean_Conv2d_4b_3x3_vggface2/n000958/epsilon_0.1/png\n",
      "Saving to /data/vggface/test_perturbed_sampled/n001781/mean_Conv2d_4b_3x3_vggface2/n003356/epsilon_0.1/png\n",
      "Saving to /data/vggface/test_perturbed_sampled/n001781/mean_Conv2d_4b_3x3_vggface2/n003356/epsilon_0.1/png\n",
      "Saving to /data/vggface/test_perturbed_sampled/n001781/mean_Conv2d_4b_3x3_vggface2/n008655/epsilon_0.1/png\n",
      "Saving to /data/vggface/test_perturbed_sampled/n001781/mean_Conv2d_4b_3x3_vggface2/n008655/epsilon_0.1/png\n",
      "Saving to /data/vggface/test_perturbed_sampled/n001781/mean_Conv2d_4b_3x3_vggface2/n008613/epsilon_0.1/png\n",
      "Saving to /data/vggface/test_perturbed_sampled/n001781/mean_Conv2d_4b_3x3_vggface2/n008613/epsilon_0.1/png\n",
      "Saving to /data/vggface/test_perturbed_sampled/n001781/mean_Conv2d_4b_3x3_vggface2/n004658/epsilon_0.1/png\n",
      "Saving to /data/vggface/test_perturbed_sampled/n001781/mean_Conv2d_4b_3x3_vggface2/n004658/epsilon_0.1/png\n",
      "Saving to /data/vggface/test_perturbed_sampled/n001781/mean_Conv2d_4b_3x3_vggface2/n001683/epsilon_0.1/png\n",
      "Saving to /data/vggface/test_perturbed_sampled/n001781/mean_Conv2d_4b_3x3_vggface2/n001683/epsilon_0.1/png\n",
      "Saving to /data/vggface/test_perturbed_sampled/n001781/mean_Conv2d_4b_3x3_vggface2/n002647/epsilon_0.1/png\n",
      "Saving to /data/vggface/test_perturbed_sampled/n001781/mean_Conv2d_4b_3x3_vggface2/n002647/epsilon_0.1/png\n",
      "Saving to /data/vggface/test_perturbed_sampled/n001781/mean_Conv2d_4b_3x3_vggface2/n009288/epsilon_0.1/png\n",
      "Saving to /data/vggface/test_perturbed_sampled/n001781/mean_Conv2d_4b_3x3_vggface2/n009288/epsilon_0.1/png\n",
      "Saving to /data/vggface/test_perturbed_sampled/n001781/mean_Conv2d_4b_3x3_vggface2/n005427/epsilon_0.1/png\n",
      "Saving to /data/vggface/test_perturbed_sampled/n001781/mean_Conv2d_4b_3x3_vggface2/n005427/epsilon_0.1/png\n",
      "Saving to /data/vggface/test_perturbed_sampled/n001781/mean_Conv2d_4b_3x3_vggface2/n002763/epsilon_0.1/png\n",
      "Saving to /data/vggface/test_perturbed_sampled/n001781/mean_Conv2d_4b_3x3_vggface2/n002763/epsilon_0.1/png\n",
      "Saving to /data/vggface/test_perturbed_sampled/n001781/mean_Conv2d_4b_3x3_vggface2/n002503/epsilon_0.1/png\n",
      "Saving to /data/vggface/test_perturbed_sampled/n001781/mean_Conv2d_4b_3x3_vggface2/n002503/epsilon_0.1/png\n",
      "Saving to /data/vggface/test_perturbed_sampled/n001781/mean_Conv2d_4b_3x3_vggface2/n003215/epsilon_0.1/png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  5%|â–Œ         | 1/19 [01:08<20:28, 68.27s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving to /data/vggface/test_perturbed_sampled/n001781/mean_Conv2d_4b_3x3_vggface2/n003215/epsilon_0.1/png\n",
      "Saving to /data/vggface/test_perturbed_sampled/n001781/mean_Conv2d_4b_3x3_vggface2/n005359/epsilon_0.1/png\n",
      "Saving to /data/vggface/test_perturbed_sampled/n001781/mean_Conv2d_4b_3x3_vggface2/n005359/epsilon_0.1/png\n",
      "Saving to /data/vggface/test_perturbed_sampled/n001781/mean_Conv2d_4b_3x3_vggface2/n005303/epsilon_0.1/png\n",
      "Saving to /data/vggface/test_perturbed_sampled/n001781/mean_Conv2d_4b_3x3_vggface2/n005303/epsilon_0.1/png\n",
      "Saving to /data/vggface/test_perturbed_sampled/n001781/mean_Conv2d_4b_3x3_vggface2/n007548/epsilon_0.1/png\n",
      "Saving to /data/vggface/test_perturbed_sampled/n001781/mean_Conv2d_4b_3x3_vggface2/n007548/epsilon_0.1/png\n",
      "Saving to /data/vggface/test_perturbed_sampled/n001781/mean_Conv2d_4b_3x3_vggface2/n000029/epsilon_0.1/png\n",
      "Saving to /data/vggface/test_perturbed_sampled/n001781/mean_Conv2d_4b_3x3_vggface2/n000029/epsilon_0.1/png\n",
      "Saving to /data/vggface/test_perturbed_sampled/n001781/mean_Conv2d_4b_3x3_vggface2/n009114/epsilon_0.1/png\n",
      "Saving to /data/vggface/test_perturbed_sampled/n001781/mean_Conv2d_4b_3x3_vggface2/n009114/epsilon_0.1/png\n",
      "input images mean -1.4481720711501112e-08 and stddev 0.9999998211860657\n",
      "INFO:tensorflow:Scale of 0 disables regularizer.\n",
      "INFO:tensorflow:Restoring parameters from /home/ivan/facenet/models/20180402-114759/model-20180402-114759.ckpt-275\n"
     ]
    }
   ],
   "source": [
    "for epsilon in [0.1, 0.25, 0.5]:\n",
    "    generate_decoys_bigger_batches(\n",
    "        people,\n",
    "        \"mean_{}\".format(layer_to_target),\n",
    "        \"vggface2\",\n",
    "        0.01,\n",
    "        epsilon,\n",
    "        2000,\n",
    "        layer_to_target\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epsilon in [0.1, 0.25, 0.5]:\n",
    "    generate_decoys_bigger_batches(\n",
    "        people,\n",
    "        \"mean_{}\".format(layer_to_target),\n",
    "        \"casia-webface\",\n",
    "        0.01,\n",
    "        epsilon,\n",
    "        2000,\n",
    "        layer_to_target\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FacialVectorsDatabase:\n",
    "    def __init__(self, folder, model_name, epsilon, attack_strategy, image_format, num_clean, include_decoys=True):\n",
    "        self.associated_identities = []\n",
    "        self.vectors = []\n",
    "        self.associated_paths = []\n",
    "        \n",
    "        identities = os.listdir(folder)\n",
    "        \n",
    "        \n",
    "        self.graph = tf.Graph()\n",
    "        \n",
    "        with self.graph.as_default():\n",
    "            self.model = build_model(model_name)\n",
    "\n",
    "            for person_name in tqdm(identities):\n",
    "                attack_subfolder = os.path.join(folder, person_name, attack_strategy)\n",
    "\n",
    "                protected = os.listdir(attack_subfolder)\n",
    "                \n",
    "                if include_decoys:\n",
    "                    for indx, other_identity in enumerate(protected):\n",
    "                        protected_folder = os.path.join(\n",
    "                            attack_subfolder, \n",
    "                            other_identity, \n",
    "                            \"epsilon_{eps}\".format(eps=epsilon), \n",
    "                            image_format\n",
    "                        )\n",
    "\n",
    "                        self._add_folder_for_person(\n",
    "                            protected_folder, \n",
    "                            person_name,\n",
    "                            exclude_endings=None,\n",
    "                            max_imgs=-1\n",
    "                        )\n",
    "                        \n",
    "                # These were the images we used for this person for adversarial modification\n",
    "                used_images = [\n",
    "                    x.split(\"/\")[-1] \\\n",
    "                    for indx, x in enumerate(self.associated_paths) \\\n",
    "                    if self.associated_identities[indx] == person_name\n",
    "                ]\n",
    "                \n",
    "                clean_folder = os.path.join(\n",
    "                    folder, person_name, \"community_naive_mean\", protected[0], \"epsilon_0.0\", \"png\")\n",
    "                self._add_folder_for_person(\n",
    "                    clean_folder, \n",
    "                    person_name,\n",
    "                    exclude_endings=set(used_images),\n",
    "                    max_imgs=num_clean\n",
    "                )\n",
    "        \n",
    "                \n",
    "    def _load_one_facenet(self, path, crop_box=None, resize_size=None, prewhiten_img=True):\n",
    "        img = Image.open(path).convert(\"RGB\")\n",
    "        \n",
    "        if crop_box:\n",
    "            img = img.crop(crop_box)\n",
    "            \n",
    "        if resize_size:\n",
    "            img = img.resize((resize_size, resize_size))\n",
    "            \n",
    "        img = (np.array(img)).astype(np.float32)\n",
    "        \n",
    "        if prewhiten_img:\n",
    "            img = prewhiten(img)\n",
    "            \n",
    "        return img\n",
    "        \n",
    "\n",
    "    def _load_folder_for_facenet(self, folder, exclude_endings=None, max_imgs=-1):\n",
    "        paths_list = glob.glob(os.path.join(folder, \"*\"))\n",
    "        len_before = len(paths_list)\n",
    "        if not (exclude_endings is None):\n",
    "            paths_list = [x for x in paths_list if not (x.split(\"/\")[-1] in exclude_endings)]\n",
    "            \n",
    "        if max_imgs > 0:\n",
    "            paths_list = paths_list[:max_imgs]\n",
    "        \n",
    "        final_imgs = [\n",
    "            self._load_one_facenet(\n",
    "                img_path, \n",
    "                prewhiten_img=False, \n",
    "                resize_size=None, \n",
    "                crop_box=None) for img_path in paths_list\n",
    "        ]\n",
    "\n",
    "        final_imgs = np.array(final_imgs)\n",
    "        final_imgs = prewhiten(np.array(final_imgs))\n",
    "        return final_imgs, paths_list\n",
    "\n",
    "    \n",
    "    def _compute_embeddings(self, images):\n",
    "        return self.model.predict(np.array(images), batch_size=len(images))\n",
    "    \n",
    "    \n",
    "    def _add_folder_for_person(self, folder, person_name, exclude_endings=None, max_imgs=-1):\n",
    "        images, paths_list = self._load_folder_for_facenet(folder, exclude_endings, max_imgs=max_imgs)\n",
    "        \n",
    "        if images is None:\n",
    "            return\n",
    "        \n",
    "        vectors = self._compute_embeddings(images)\n",
    "        self.vectors.extend(vectors)\n",
    "        self.associated_identities.extend([person_name for _ in range(len(vectors))])\n",
    "        self.associated_paths.extend(paths_list)\n",
    "    \n",
    "    def nearest_neighbor_to_img_at_path(self, query_path):\n",
    "        with self.graph.as_default():\n",
    "            query_vector = self._compute_embeddings(\n",
    "                np.expand_dims(self._load_one_facenet(query_path, resize_size=160), axis=0))\n",
    "        distances = pairwise_distances(query_vector, self.vectors, metric=\"cosine\")[0]\n",
    "        min_dist_indx = np.argmin(distances)\n",
    "        return self.associated_identities[min_dist_indx], self.associated_paths[min_dist_indx]\n",
    "    \n",
    "\n",
    "def full_path_to_info(path):\n",
    "    split = path.split(\"/\")\n",
    "    if path.startswith(\"/data/vggface/test_perturbed_sampled\"):\n",
    "        # split of path breaks down like this:\n",
    "        #     0 \n",
    "        #     1 data\n",
    "        #     2 vggface\n",
    "        #     3 test_perturbed_sampled\n",
    "        #     4 {protector}\n",
    "        #     5 community_naive_mean\n",
    "        #     6 {protected}\n",
    "        #     7 epsilon_0.0\n",
    "        #     8 png\n",
    "        #     9 35.png\n",
    "        return \"-\".join([split[4], split[6], split[7], split[9]])\n",
    "    else:\n",
    "        # split of path breaks down like this:\n",
    "        #     0 \n",
    "        #     1 data\n",
    "        #     2 vggface\n",
    "        #     3 test_query_antisampled\n",
    "        #     4 {protector}\n",
    "        #     5 image_name.jpeg\n",
    "        return \"-\".join([split[4], split[5]])\n",
    "    \n",
    "    \n",
    "def measure_local_recall(\n",
    "    faces_database,\n",
    "    image_directory=\"/data/vggface/test_query_antisampled\",\n",
    "    num_query=10,\n",
    "    verbose=False\n",
    "):\n",
    "    discovery = []\n",
    "    true = []\n",
    "    identified_as = []\n",
    "    \n",
    "    paths_of_query = []\n",
    "    paths_of_nearest = []\n",
    "\n",
    "    for protector in os.listdir(image_directory):\n",
    "        # We are sourcing query photos from epsilon_0.0.\n",
    "        # In those cases, all subfolders in the \"protected\" identity have the same, clean\n",
    "        # photo of the protector, so we just pick any single one that exists (e.g. n000958)\n",
    "        # For the case where n000958 is itself the protector, n000958 is not present in its protected\n",
    "        # subfolders, so we pick n000029 without loss of generality.\n",
    "        if protector == \"n000958\":\n",
    "            protected = \"n000029\"\n",
    "        else:\n",
    "            protected = \"n000958\"\n",
    "\n",
    "        query_photos_paths = sorted(glob.glob(\n",
    "            os.path.join(image_directory, protector, \"*\")\n",
    "        ))\n",
    "\n",
    "        \n",
    "        for i in np.random.choice(len(query_photos_paths), num_query):\n",
    "            chosen_path = query_photos_paths[i]\n",
    "            top_identity, top_identity_path = faces_database.nearest_neighbor_to_img_at_path(chosen_path)\n",
    "            \n",
    "            paths_of_query.append(full_path_to_info(chosen_path))\n",
    "            paths_of_nearest.append(full_path_to_info(top_identity_path))\n",
    "            \n",
    "            true.append(protector)\n",
    "            identified_as.append(top_identity)\n",
    "\n",
    "            if top_identity == protector:\n",
    "                discovery.append(1.0)\n",
    "            else:\n",
    "                discovery.append(0.0)\n",
    "\n",
    "    if verbose:\n",
    "        for true_id, recognized_id, query, nearest in zip(true, identified_as, paths_of_query, paths_of_nearest):\n",
    "            print(\"Face of {true_id} identitifed as {recognized_id}. Nearest neighbor to {query} was {nearest}\".format(\n",
    "                true_id=true_id, recognized_id=recognized_id, query=query, nearest=nearest))\n",
    "        \n",
    "    return sum(discovery)/len(discovery)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_recall_once(\n",
    "    network_to_evaluate,\n",
    "    epsilon,\n",
    "    attack_name,\n",
    "    num_clean,\n",
    "    include_adversarial,\n",
    "    run\n",
    "):\n",
    "    faces_db = FacialVectorsDatabase(\n",
    "        folder=\"/data/vggface/test_perturbed_sampled\",\n",
    "        model_name=network_to_evaluate,\n",
    "        epsilon=epsilon,\n",
    "        attack_strategy=attack_name,\n",
    "        image_format=\"png\", \n",
    "        num_clean=num_clean,\n",
    "        include_decoys=include_adversarial\n",
    "    )\n",
    "    recall_at_k1 = measure_local_recall(faces_db, num_query=10, verbose=False)\n",
    "\n",
    "    faces_db.model.sess.close()\n",
    "    \n",
    "    return {\n",
    "        \"num_clean\": num_clean, \n",
    "        \"run\": run,\n",
    "        \"recall\": recall_at_k1,\n",
    "        \"epsilon\": epsilon\n",
    "    }\n",
    "\n",
    "def generate_results_plot(\n",
    "    network_to_evaluate,\n",
    "    attack_name\n",
    "):\n",
    "    results = []\n",
    "    \n",
    "    # get a run with all clean ones and no decous\n",
    "    res = get_recall_once(\n",
    "        network_to_evaluate,\n",
    "        0.5, # epsilon is irrelevant\n",
    "        attack_name,\n",
    "        -1, # negative or 0 number includes all\n",
    "        False,\n",
    "        11\n",
    "    )\n",
    "    results.append(res)\n",
    "    \n",
    "    for epsilon in [0.1, 0.25, 0.5]:\n",
    "        for num_clean in range(1, 10, 2):\n",
    "            for run in range(10):\n",
    "                res = get_recall_once(\n",
    "                    network_to_evaluate,\n",
    "                    epsilon,\n",
    "                    attack_name,\n",
    "                    num_clean,\n",
    "                    True,\n",
    "                    run\n",
    "                )\n",
    "                results.append(res)\n",
    "                \n",
    "    results_pd = pd.DataFrame(results)\n",
    "\n",
    "    results_pd[\"decoys_vs_clean\"] = 36 / results_pd[\"num_clean\"]\n",
    "    results_pd[results_pd.decoys_vs_clean < 0.0][\"decoys_vs_clean\"] = 0.0\n",
    "    \n",
    "    results_pd.to_csv(\n",
    "        \"/home/ivan/pascal_adversarial_faces/results/recall_at_1_{attack_name}_{network_to_evaluate}.csv\".format(\n",
    "            network_to_evaluate=network_to_evaluate,\n",
    "            attack_name=attack_name\n",
    "        ))\n",
    "\n",
    "    ax = sns.lineplot(\n",
    "        data=results_pd, x=\"decoys_vs_clean\", y=\"recall\", style=\"epsilon\", markers=True)\n",
    "    ax.set_title(\"Attack {attack} evaluated on {network_to_evaluate}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_results_plot(\n",
    "    \"casia-webface\",\n",
    "    \"mean_{}_vggface2\".format(layer_to_target)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_results_plot(\n",
    "    \"vggface2\",\n",
    "    \"mean_{}_casia-webface\".format(layer_to_target)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_results_plot(\n",
    "    \"vggface2\",\n",
    "    \"mean_{}_vggface2\".format(layer_to_target)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_results_plot(\n",
    "    \"casia-webface\",\n",
    "    \"mean_{}_casia-webface\".format(layer_to_target)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
