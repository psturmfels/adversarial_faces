{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "import tensorflow as tf\n",
    "import glob\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from sklearn.metrics import pairwise_distances, roc_auc_score\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline\n",
    "from tqdm import tqdm\n",
    "import sys\n",
    "sys.path.append(\"../\")\n",
    "from inception_resnet_v1 import inference\n",
    "\n",
    "import seaborn as sns\n",
    "\n",
    "import traceback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prewhiten(x):\n",
    "    \"\"\"\n",
    "    A helper function to whiten an image, or a batch of images.\n",
    "    Args:\n",
    "        x: An image or batch of images.\n",
    "    \"\"\"\n",
    "    if x.ndim == 4:\n",
    "        axis = (1, 2, 3)\n",
    "        size = x[0].size\n",
    "    elif x.ndim == 3:\n",
    "        axis = (0, 1, 2)\n",
    "        size = x.size\n",
    "    else:\n",
    "        print(x.ndim)\n",
    "        raise ValueError('Dimension should be 3 or 4')\n",
    "\n",
    "    mean = np.mean(x, axis=axis, keepdims=True)\n",
    "    std = np.std(x, axis=axis, keepdims=True)\n",
    "#     std_adj = np.maximum(std, 1.0/np.sqrt(size))\n",
    "    y = (x - mean) / std\n",
    "    return y\n",
    "\n",
    "def l2_normalize(x, axis=-1, epsilon=1e-10):\n",
    "    \"\"\"\n",
    "    Normalizes an embedding to have unit length in the l2 metric.\n",
    "    Args:\n",
    "        x: A batch of numpy embeddings\n",
    "    \"\"\"\n",
    "    output = x / np.sqrt(np.maximum(np.sum(np.square(x),\n",
    "                                           axis=axis,\n",
    "                                           keepdims=True),\n",
    "                                    epsilon))\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PersonImages:\n",
    "    def __init__(self, person_name):\n",
    "        self.clean_folder = \"\"\n",
    "        self.clean_images = []\n",
    "        self.adversarial_images = []\n",
    "        self.orig_mean = None\n",
    "        self.orig_std = None\n",
    "        self.orig_paths = []\n",
    "        self.person_name = person_name\n",
    "        self.clean_embeddings = {}\n",
    "    \n",
    "    def _load_one_facenet(self, path, resize_size=None, prewhiten_img=True):\n",
    "        img = Image.open(path).convert(\"RGB\")\n",
    "    \n",
    "            \n",
    "        if resize_size:\n",
    "            img = img.resize((resize_size, resize_size))\n",
    "            \n",
    "        img = (np.array(img)).astype(np.float32)\n",
    "        \n",
    "        if prewhiten_img:\n",
    "            img = prewhiten(img)\n",
    "            \n",
    "        return img\n",
    "        \n",
    "    def _load_folder_for_facenet(self, folder, resize_size=None):\n",
    "        paths_list = glob.glob(os.path.join(folder, \"*\"))\n",
    "        final_imgs = []\n",
    "        for img_path in paths_list:\n",
    "            final_imgs.append(\n",
    "                self._load_one_facenet(\n",
    "                    img_path, prewhiten_img=False, resize_size=resize_size))\n",
    "        \n",
    "        final_imgs = np.array(final_imgs)\n",
    "        mean, std = np.mean(final_imgs), np.std(final_imgs)\n",
    "        final_imgs = prewhiten(np.array(final_imgs))\n",
    "        return final_imgs, mean, std, paths_list\n",
    "        \n",
    "    def load_clean_from_folder(self, clean_folder, resize_size=160):\n",
    "        self.clean_folder = clean_folder\n",
    "        self.clean_images, self.orig_mean, self.orig_std, self.orig_paths = self._load_folder_for_facenet(\n",
    "            clean_folder, resize_size=resize_size)\n",
    "    \n",
    "    def _undo_preprocess(self, images):\n",
    "        restored_images = images.copy()\n",
    "        restored_images  *= self.orig_std\n",
    "        restored_images += self.orig_mean\n",
    "        restored_images = np.clip(restored_images, 0.0, 255.0)\n",
    "        return np.uint8(restored_images)\n",
    "    \n",
    "    def _compute_embeddings(self, model, images):\n",
    "        return model.predict(np.array(images), batch_size=len(images))\n",
    "    \n",
    "    def get_clean_for_display(self):\n",
    "        return self._undo_preprocess(self.clean_images)\n",
    "    \n",
    "    def compute_clean_embeddings_with_model(self, model, model_name):\n",
    "        self.clean_embeddings[model_name] = self._compute_embeddings(model, self.clean_images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyModel:\n",
    "    def __init__(self, model_path, inputs=None, sess=None, graph=None):\n",
    "        if graph is None:\n",
    "            print(\"graph is None, creating new graph\")\n",
    "            self.graph = tf.Graph()\n",
    "        else:\n",
    "            self.graph = graph\n",
    "            \n",
    "        with self.graph.as_default():\n",
    "            if inputs is None:\n",
    "                self.model_inputs = tf.placeholder(tf.float32, shape=(None, 160, 160, 3))\n",
    "            else:\n",
    "                self.model_inputs = inputs\n",
    "\n",
    "            vars_before = tf.global_variables()\n",
    "            self.net, self.endpoints = inference(\n",
    "                self.model_inputs, keep_probability=1.0, bottleneck_layer_size=512, phase_train=False)\n",
    "            vars_after = tf.global_variables()\n",
    "\n",
    "            model_name = dataset_to_model_name[\"vggface2\"]\n",
    "            model_vars = list(set(vars_after) - set(vars_before))\n",
    "            model_vars_dict = {}\n",
    "\n",
    "            curr_scope_name = tf.get_variable_scope().name\n",
    "\n",
    "            for v in model_vars:\n",
    "                if v.name.startswith(\"InceptionResnetV1\"):\n",
    "                    name_without_scope_prefix = v.name[:-2]\n",
    "                else:\n",
    "                    name_without_scope_prefix = v.name[len(curr_scope_name)+ 1:-2]\n",
    "\n",
    "                model_vars_dict[name_without_scope_prefix] = v\n",
    "\n",
    "            saver = tf.train.Saver(var_list=model_vars_dict)\n",
    "            \n",
    "            if sess is None:\n",
    "                print(\"sess is None, creating new sess\")\n",
    "                self.sess = tf.Session(graph=graph)\n",
    "            else:\n",
    "                self.sess = sess\n",
    "                \n",
    "            saver.restore(self.sess, model_path)\n",
    "                \n",
    "    \n",
    "    def predict(self, inputs, batch_size):\n",
    "        with self.graph.as_default():\n",
    "            return self.sess.run(self.net, feed_dict={self.model_inputs: inputs})\n",
    "    \n",
    "#     def __del__(self):\n",
    "#         print(\"deleting model and closing session\")\n",
    "#         print(traceback.print_stack())\n",
    "#         self.sess.close()\n",
    "\n",
    "dataset_to_model_name = {\n",
    "    \"casia-webface\": \"20180408-102900\",\n",
    "    \"vggface2\": \"20180402-114759\"\n",
    "}\n",
    "\n",
    "dataset_to_ckpt_number = {\n",
    "    \"casia-webface\": \"90\",\n",
    "    \"vggface2\": \"275\"\n",
    "}\n",
    "\n",
    "def build_model(dataset_name, inputs=None, sess=None, graph=None):\n",
    "    model_name = dataset_to_model_name[dataset_name]\n",
    "    model = MyModel(\n",
    "        os.path.join(\n",
    "            \"/home/ivan/facenet/models\",\n",
    "            model_name,\n",
    "            \"model-{model_name}.ckpt-{ckpt_num}\".format(\n",
    "                model_name=dataset_to_model_name[dataset_name],\n",
    "                ckpt_num=dataset_to_ckpt_number[dataset_name]\n",
    "            )),\n",
    "        inputs, \n",
    "        sess,\n",
    "        graph\n",
    "    )\n",
    "    return model\n",
    "\n",
    "def build_vggface_model(inputs, sess, graph):\n",
    "    return build_model(\"vggface2\", inputs, sess, graph)\n",
    "\n",
    "def build_casiawebface_model(inputs, sess, graph):\n",
    "    return build_model(\"casia-webface\", inputs, sess, graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def path_for_id_clean(identity):\n",
    "    if identity == \"n000958\":\n",
    "        return \"/data/vggface/test_perturbed_sampled/{id}/community_naive_mean/n000029/epsilon_0.0/png\".format(id=identity)\n",
    "    else:\n",
    "        return \"/data/vggface/test_perturbed_sampled/{id}/community_naive_mean/n000958/epsilon_0.0/png\".format(id=identity)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_person(person_name, path_for_id_fn, models_ls, model_names_ls):\n",
    "    person_a = PersonImages(person_name)\n",
    "    person_a.load_clean_from_folder(path_for_id_fn(person_a.person_name))\n",
    "    \n",
    "    for model, model_name in zip(models_ls, model_names_ls):\n",
    "        print(\"Computing embeddings for\", person_name, \"with model\", model_name)\n",
    "        person_a.compute_clean_embeddings_with_model(model, model_name)\n",
    "    return person_a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def attack_batch_cw(\n",
    "    build_model_fn_ls, \n",
    "    model_names_ls,\n",
    "    input_images,\n",
    "    target_vectors_ls,\n",
    "    learning_rate,\n",
    "    epsilon,\n",
    "    max_iters\n",
    "):\n",
    "    input_images = np.array(input_images)\n",
    "    batch_size, orig_h, orig_w, orig_c = input_images.shape\n",
    "    \n",
    "    graph = tf.Graph()\n",
    "    \n",
    "    with graph.as_default():\n",
    "        with tf.Session() as sess:\n",
    "            images_input_ph = tf.placeholder(\n",
    "                tf.float32,\n",
    "                name=\"input_images\",\n",
    "                shape=input_images.shape\n",
    "            )\n",
    "            \n",
    "            noise_var = tf.get_variable(\n",
    "                name=\"adversarial_noise\",\n",
    "                shape=input_images.shape,\n",
    "                initializer=tf.initializers.truncated_normal(\n",
    "                    mean=np.mean(input_images),\n",
    "                    stddev=3 * np.std(input_images)\n",
    "                )\n",
    "            )\n",
    "            sess.run(noise_var.initializer)\n",
    "            \n",
    "            images_plus_noise = images_input_ph + noise_var\n",
    "\n",
    "            randomized_images_plus_noise = tf.image.random_brightness(\n",
    "                images_plus_noise, 0.5)\n",
    "        \n",
    "            randomized_images_plus_noise = tf.image.random_crop(\n",
    "                randomized_images_plus_noise, \n",
    "                [batch_size, orig_h - 10, orig_w - 10, 3]\n",
    "            )\n",
    "\n",
    "            randomized_images_plus_noise = tf.image.resize_images(\n",
    "                randomized_images_plus_noise, [orig_h, orig_w])\n",
    "            \n",
    "            randomized_images_plus_noise += tf.random.normal(\n",
    "                randomized_images_plus_noise.shape, 0.0, 0.75)\n",
    "            \n",
    "            randomized_images_plus_noise = tf.clip_by_value(\n",
    "                randomized_images_plus_noise, input_images - epsilon, input_images + epsilon)\n",
    "            \n",
    "            total_loss = 0.0\n",
    "            for target_vectors, build_model_fn, model_name in zip(\n",
    "                target_vectors_ls, build_model_fn_ls, model_names_ls):\n",
    "                                \n",
    "                with tf.variable_scope(model_name):\n",
    "                    target_vectors = np.array(target_vectors)\n",
    "                    print(\"shape of targets\", target_vectors.shape)\n",
    "                    targets_var = tf.get_variable(\n",
    "                        name=\"targets\",\n",
    "                        shape=np.array(target_vectors).shape,\n",
    "                    )\n",
    "                    sess.run(tf.assign(targets_var, target_vectors))\n",
    "                    \n",
    "                    model = build_model_fn(\n",
    "                        inputs=randomized_images_plus_noise, \n",
    "                        sess=sess,\n",
    "                        graph=graph\n",
    "                    )\n",
    "            \n",
    "                    model_outputs = tf.nn.l2_normalize(model.net, axis=1)\n",
    "                    targets = tf.nn.l2_normalize(targets_var, axis=1)\n",
    "                    loss = tf.losses.cosine_distance(targets, model_outputs, axis=1)\n",
    "                    total_loss += loss\n",
    "                print(\"Loaded model\", model_name, \"for decoy generation\")\n",
    "\n",
    "            total_loss += 1e-6 * tf.nn.l2_loss(noise_var)\n",
    "            \n",
    "            vars_before = set(tf.global_variables())\n",
    "            train_op = tf.train.AdamOptimizer(learning_rate).minimize(\n",
    "                total_loss, var_list=[noise_var])\n",
    "            vars_after = set(tf.global_variables())\n",
    "            sess.run([v.initializer for v in list(vars_after - vars_before)])\n",
    "            \n",
    "            \n",
    "            losses = []\n",
    "            for i in range(max_iters):\n",
    "                loss_value, total_loss_value, _ = sess.run(\n",
    "                    [loss, total_loss, train_op], feed_dict={images_input_ph: input_images})\n",
    "                assert not np.isnan(loss_value), \"Loss_value is nan\"\n",
    "                losses.append(loss_value)\n",
    "            \n",
    "            final_imgs = sess.run(\n",
    "                tf.clip_by_value(images_plus_noise, input_images - epsilon, input_images + epsilon),\n",
    "                feed_dict={images_input_ph: input_images}\n",
    "            )\n",
    "            \n",
    "            return final_imgs, losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ensemble_generate_decoys_bigger_batches(\n",
    "    attack_strategy,\n",
    "    learning_rate,\n",
    "    epsilon,\n",
    "    max_iters\n",
    "):\n",
    "    model_names = [\"vggface2\", \"casia-webface\"]\n",
    "    model_build_fn_ls = [build_vggface_model, build_casiawebface_model]\n",
    "    \n",
    "    built_models_for_embeddings = []\n",
    "    for model_to_attack in model_names:\n",
    "        built_models_for_embeddings.append(build_model(model_to_attack))\n",
    "        \n",
    "    people_list = [\n",
    "        build_person(person_name, path_for_id_clean, built_models_for_embeddings, model_names) \\\n",
    "        for person_name in os.listdir(\"/data/vggface/test_perturbed_sampled\")\n",
    "    ]\n",
    "    \n",
    "    for model in built_models_for_embeddings:\n",
    "        model.sess.close()\n",
    "    \n",
    "    print(\"Done building people objects and computing embeddings.\")\n",
    "    \n",
    "    for person in tqdm(people_list):\n",
    "        num, height, width, channels = person.clean_images.shape\n",
    "        person.adversarial_images = np.zeros((2*(len(people_list) - 1), height, width, channels))\n",
    "        \n",
    "        indx = 0\n",
    "        \n",
    "        images_to_make_adversarial = []\n",
    "        targets_for_images = []\n",
    "        target_vectors_ls = [[], []]\n",
    "        \n",
    "        for other_person in people_list:\n",
    "            if person.person_name == other_person.person_name:\n",
    "                continue\n",
    "            \n",
    "            \n",
    "            current_chosen_indices = range(indx, indx + 2)\n",
    "            images_to_make_adversarial.extend(np.take(\n",
    "                person.clean_images, current_chosen_indices, axis=0))\n",
    "            \n",
    "            for indx, model_name in enumerate(model_names):\n",
    "                target_vector = np.mean(other_person.clean_embeddings[model_name], axis=0)\n",
    "                target_vectors_ls[indx].extend(np.array([\n",
    "                    target_vector for _ in range(len(current_chosen_indices))]))\n",
    "            \n",
    "            indx += 2\n",
    "            \n",
    "        images_to_make_adversarial = np.array(images_to_make_adversarial)\n",
    "        \n",
    "        all_adversarial_images, losses =attack_batch_cw(\n",
    "            model_build_fn_ls, \n",
    "            model_names,\n",
    "            np.array(images_to_make_adversarial),\n",
    "            target_vectors_ls,\n",
    "            learning_rate,\n",
    "            epsilon,\n",
    "            max_iters\n",
    "        )\n",
    "        \n",
    "            \n",
    "        fig, ax = plt.subplots()\n",
    "        ax.plot(range(len(losses)), losses)\n",
    "        plt.show()\n",
    "        \n",
    "        indx = 0\n",
    "        for other_person in people_list:\n",
    "            if person.person_name == other_person.person_name:\n",
    "                continue\n",
    "            \n",
    "            save_dest = os.path.join(\n",
    "                \"/data/vggface/test_perturbed_sampled\",\n",
    "                person.person_name,\n",
    "                \"{attack_strategy}_{model}\".format(attack_strategy=attack_strategy, model=model_to_attack),\n",
    "                other_person.person_name\n",
    "            )\n",
    "            \n",
    "            save_path = os.path.join(save_dest, \"epsilon_{}\".format(epsilon), \"png\")\n",
    "            os.makedirs(save_path, exist_ok=True)\n",
    "            existing_files = os.listdir(save_path)\n",
    "\n",
    "            # Clean up folder if need be\n",
    "            if len(existing_files) > 0:\n",
    "                for f in existing_files:\n",
    "                    os.remove(os.path.join(save_path, f))\n",
    "            \n",
    "            for i in range(indx, indx + 2):\n",
    "                orig_name = person.orig_paths[i].split(\"/\")[-1]\n",
    "                person.adversarial_images[i] = all_adversarial_images[i]\n",
    "                full_save_path = os.path.join(save_path, orig_name)\n",
    "                print(\"Saving to\", save_path)\n",
    "                Image.fromarray(\n",
    "                    person._undo_preprocess(person.adversarial_images[i])\n",
    "                ).save(full_save_path)\n",
    "\n",
    "            indx += 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "graph is None, creating new graph\n",
      "INFO:tensorflow:Scale of 0 disables regularizer.\n",
      "sess is None, creating new sess\n",
      "INFO:tensorflow:Restoring parameters from /home/ivan/facenet/models/20180402-114759/model-20180402-114759.ckpt-275\n",
      "graph is None, creating new graph\n",
      "INFO:tensorflow:Scale of 0 disables regularizer.\n",
      "sess is None, creating new sess\n",
      "INFO:tensorflow:Restoring parameters from /home/ivan/facenet/models/20180408-102900/model-20180408-102900.ckpt-90\n",
      "Computing embeddings for n001781 with model vggface2\n",
      "Computing embeddings for n001781 with model casia-webface\n",
      "Computing embeddings for n009232 with model vggface2\n",
      "Computing embeddings for n009232 with model casia-webface\n",
      "Computing embeddings for n000958 with model vggface2\n",
      "Computing embeddings for n000958 with model casia-webface\n",
      "Computing embeddings for n003356 with model vggface2\n",
      "Computing embeddings for n003356 with model casia-webface\n",
      "Computing embeddings for n008655 with model vggface2\n",
      "Computing embeddings for n008655 with model casia-webface\n",
      "Computing embeddings for n008613 with model vggface2\n",
      "Computing embeddings for n008613 with model casia-webface\n",
      "Computing embeddings for n004658 with model vggface2\n",
      "Computing embeddings for n004658 with model casia-webface\n",
      "Computing embeddings for n001683 with model vggface2\n",
      "Computing embeddings for n001683 with model casia-webface\n",
      "Computing embeddings for n002647 with model vggface2\n",
      "Computing embeddings for n002647 with model casia-webface\n",
      "Computing embeddings for n009288 with model vggface2\n",
      "Computing embeddings for n009288 with model casia-webface\n",
      "Computing embeddings for n005427 with model vggface2\n",
      "Computing embeddings for n005427 with model casia-webface\n",
      "Computing embeddings for n002763 with model vggface2\n",
      "Computing embeddings for n002763 with model casia-webface\n",
      "Computing embeddings for n002503 with model vggface2\n",
      "Computing embeddings for n002503 with model casia-webface\n",
      "Computing embeddings for n003215 with model vggface2\n",
      "Computing embeddings for n003215 with model casia-webface\n",
      "Computing embeddings for n005359 with model vggface2\n",
      "Computing embeddings for n005359 with model casia-webface\n",
      "Computing embeddings for n005303 with model vggface2\n",
      "Computing embeddings for n005303 with model casia-webface\n",
      "Computing embeddings for n007548 with model vggface2\n",
      "Computing embeddings for n007548 with model casia-webface\n",
      "Computing embeddings for n000029 with model vggface2\n",
      "Computing embeddings for n000029 with model casia-webface\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/19 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing embeddings for n009114 with model vggface2\n",
      "Computing embeddings for n009114 with model casia-webface\n",
      "Done building people objects and computing embeddings.\n",
      "shape of targets (36, 512)\n",
      "INFO:tensorflow:Scale of 0 disables regularizer.\n",
      "INFO:tensorflow:Restoring parameters from /home/ivan/facenet/models/20180402-114759/model-20180402-114759.ckpt-275\n",
      "Loaded model vggface2 for decoy generation\n",
      "shape of targets (36, 512)\n",
      "INFO:tensorflow:Scale of 0 disables regularizer.\n",
      "INFO:tensorflow:Restoring parameters from /home/ivan/facenet/models/20180408-102900/model-20180408-102900.ckpt-90\n",
      "Loaded model casia-webface for decoy generation\n"
     ]
    }
   ],
   "source": [
    "for epsilon in [0.5]:\n",
    "    ensemble_generate_decoys_bigger_batches(\n",
    "        \"ensemble\",\n",
    "        0.01,\n",
    "        epsilon,\n",
    "        2000\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
